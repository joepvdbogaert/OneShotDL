{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "The developed classes need some thorough testing before we run the whole experiment. It would be very bad if the script breaks in the middle of tuning. This notebook lists some situations that should be tested and explains how to do so.\n",
    "\n",
    "The tests can be performed by evaluating the objective function for a specified set of parameter values. Every class has a method <i>objfunction</i>, which takes a list of values as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "import helpers\n",
    "import models\n",
    "from models import OneShotCNN, OneShotTransferCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   3.00000000e+00   4.00000000e+00\n",
      "   5.00000000e+00   0.00000000e+00   0.00000000e+00   5.00000000e+01]\n"
     ]
    }
   ],
   "source": [
    "# initialize with logging and verbose\n",
    "model = OneShotCNN(log=True, verbose=1)\n",
    "# choose lower bound of model as example (THIS SHOULD BE DONE MANUALLY TO PERFORM THE TESTS)\n",
    "x = model.xlow\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, x is just a list of numbers. This can be put in the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Experiment 1.\n",
      "-------------\n",
      "learning_rate: 0.0001\n",
      "dropout_rate1: 0.0\n",
      "dropout_rate2: 0.0\n",
      "width_shift: 0.0\n",
      "height_shift: 0.0\n",
      "shear: 0.0\n",
      "zoom: 0.0\n",
      "num_conv_layers: 2.0\n",
      "num_dense_layers: 0.0\n",
      "num_maxpools: 0.0\n",
      "neurons_first_conv: 3.0\n",
      "neurons_remaining_conv: 4.0\n",
      "neurons_dense: 5.0\n",
      "rotation: 0.0\n",
      "horizontal_flip: 0.0\n",
      "epochs: 50.0\n",
      "-------------\n",
      "Cross validating..\n",
      "fit 1:\n",
      "test accuracy: 46.76%.\n",
      "fit 2:\n",
      "test accuracy: 64.38%.\n",
      "fit 3:\n",
      "test accuracy: 72.9%.\n",
      "fit 4:\n",
      "test accuracy: 50.26%.\n",
      "fit 5:\n",
      "test accuracy: 68.56%.\n",
      "fit 6:\n",
      "test accuracy: 47.32%.\n",
      "fit 7:\n",
      "test accuracy: 60.44%.\n",
      "fit 8:\n",
      "test accuracy: 45.92%.\n",
      "fit 9:\n",
      "test accuracy: 52.04%.\n",
      "fit 10:\n",
      "test accuracy: 61.28%.\n",
      "Scores: [0.46759998798370361, 0.64380002021789551, 0.72899997234344482, 0.50260001420974731, 0.68559998273849487, 0.4731999933719635, 0.60439997911453247, 0.45919999480247498, 0.52039998769760132, 0.6128000020980835].\n",
      "Mean: 56.99%. Standard deviation: 9.29%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.56985999345779415"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for given x\n",
    "model.objfunction(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <i>hyper_map</i> attribute of the model can be used to get the index of a hyperparameter in X. For example, the learning rate is the first entry of X (index 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hyper_map['learning_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see the full list of available hyperparameters (and their order), you can print the <i>hyperparams</i> attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning_rate',\n",
       " 'dropout_rate1',\n",
       " 'dropout_rate2',\n",
       " 'width_shift',\n",
       " 'height_shift',\n",
       " 'shear',\n",
       " 'zoom',\n",
       " 'num_conv_layers',\n",
       " 'num_dense_layers',\n",
       " 'num_maxpools',\n",
       " 'neurons_first_conv',\n",
       " 'neurons_remaining_conv',\n",
       " 'neurons_dense',\n",
       " 'rotation',\n",
       " 'horizontal_flip',\n",
       " 'epochs']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneShotCNN tests\n",
    "\n",
    "For the simplest class, the OneShotCNN class, we should test the following:\n",
    "- The early stopping procedure in combination with logging. Do this by training a very bad model (setting RMSProp's learning rate to 0.1 should do the trick) and have log=True on initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneShotTransferCNN tests\n",
    "\n",
    "For the simplest class, the OneShotCNN class, we should test the following:\n",
    "- The early stopping procedure in combination with logging. Do this by training a very bad model (setting RMSProp's learning rate to 0.1 should do the trick) and have log=True on initialization.\n",
    "- Having 'num_fixed_layers' larger than the total number of layers in the model.\n",
    "- .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
