{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneShotDL Prototype\n",
    "\n",
    "This notebook shows the general experiment setup and code architecture that should be used within the project. It serves as a blueprint for the more advanced models and settings that we aim to develop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s100385\\Documents\\JADS Working Files\\Research Paper One-shot\\Code\\OneShotDL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# set directory\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "from pySOT import SyncStrategyNoConstraints, LatinHypercube, RBFInterpolant, CandidateDYCORS, CubicKernel, LinearTail\n",
    "from threading import Thread, current_thread\n",
    "from datetime import datetime\n",
    "from poap.controller import ThreadController, BasicWorkerThread, SerialController\n",
    "\n",
    "# use keras for the cnn tuning example\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "\n",
    "# helper classes of OneShotDL\n",
    "from helpers import load_mnist, split_and_select_random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Minimal example of using pySOT to implement HORD\n",
    "\n",
    "We aim to use the method proposed in <i>Efficient Hyperparameter Optimization of Deep Learning Algorithms Using\n",
    "Deterministic RBF Surrogates</i> (Ilievski et al., 2017) to tune the models. The paper shows it outperforms popular Bayesian Optimization approaches like SMAC and TPE, especially when the number of parameters to tune is large.\n",
    "\n",
    "The paper: https://arxiv.org/pdf/1607.08316.pdf \n",
    "\n",
    "Their implementation uses the pySOT library. A minimal example of how to use that package is shown below. It requires a class to be written with an objective function and allowed ranges for parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Test():\n",
    "    \n",
    "    def __init__(self, dim=2):\n",
    "        # these attributes are required by pySOT\n",
    "        self.xlow = np.array([-2, 2])\n",
    "        self.xup = np.array([-0.01, 3.1])\n",
    "        self.continuous = np.arange(0,dim)\n",
    "        self.integer = np.array([])\n",
    "        self.dim = dim\n",
    "    \n",
    "        self.counter = 0\n",
    "        \n",
    "    def objfunction(self, x):\n",
    "        # a random objective function for illustration \n",
    "        # optimal solution given the ranges is 0 at x = [-2, 2]\n",
    "        self.counter += 1\n",
    "        score = 2 - np.square(x[0]) + x[1]\n",
    "        print(\"Experiment {}. Params: {}. Score: {}.\".format(self.counter, x, score))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use multi threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1. Params: [-1.83416667  2.275     ]. Score: 0.9108326388888885.\n",
      "Experiment 2. Params: [-0.17583333  2.825     ]. Score: 4.794082638888889.\n",
      "Experiment 3. Params: [-1.5025      2.45833333]. Score: 2.2008270833333334.\n",
      "Experiment 4. Params: [-0.83916667  2.09166667]. Score: 3.387465972222222.\n",
      "Experiment 5. Params: [-0.5075      3.00833333]. Score: 4.750777083333333.\n",
      "Experiment 6. Params: [-1.17083333  2.64166667]. Score: 3.270815972222222.\n",
      "Experiment 7. Params: [-1.81161083  3.04657154]. Score: 1.7646377511295581.\n",
      "Experiment 8. Params: [-1.89152746  2.01041288]. Score: 0.4325367461954621.\n",
      "Experiment 9. Params: [-1.99816961  2.14598372]. Score: 0.15330191424568396.\n",
      "Experiment 10. Params: [-1.99816961  2.02131801]. Score: 0.02863621230571889.\n",
      "Experiment 11. Params: [-1.99816961  2.66944078]. Score: 0.6767589728092176.\n",
      "Experiment 12. Params: [-1.35408338  2.00858546]. Score: 2.17504365700722.\n",
      "Experiment 13. Params: [-1.99816961  2.43979143]. Score: 0.4471096299791104.\n",
      "Experiment 14. Params: [-1.99816961  2.00303262]. Score: 0.010350822707314133.\n",
      "Experiment 15. Params: [-1.61964274  2.07598873]. Score: 1.4527461186958046.\n",
      "Experiment 16. Params: [-1.99816961  2.27764981]. Score: 0.2849680088538289.\n",
      "Experiment 17. Params: [-1.99816961  2.08034121]. Score: 0.08765940304226083.\n",
      "Experiment 18. Params: [-1.99816961  2.00681926]. Score: 0.014137457422525479.\n",
      "Experiment 19. Params: [-0.68669098  2.33464032]. Score: 3.8630958128168076.\n",
      "Experiment 20. Params: [-1.79440495  2.12037878]. Score: 0.9004896563703229.\n",
      "Experiment 21. Params: [-1.99816961  2.05110358]. Score: 0.05842177414542249.\n",
      "Experiment 22. Params: [-1.99816961  2.00093585]. Score: 0.008254046058164377.\n",
      "Experiment 23. Params: [-1.73704118  2.00093585]. Score: 0.9836237909706522.\n",
      "Experiment 24. Params: [-1.89312129  2.20660809]. Score: 0.6226998665695476.\n",
      "Experiment 25. Params: [-1.99816961  2.21101133]. Score: 0.21832952378866732.\n",
      "Experiment 26. Params: [-1.99816961  2.00526391]. Score: 0.012582109401458652.\n",
      "Experiment 27. Params: [-1.50615402  2.00093585]. Score: 1.7324359180252933.\n",
      "Experiment 28. Params: [-1.92450587  2.10343732]. Score: 0.39971447486375444.\n",
      "Experiment 29. Params: [-1.99816961  2.03577252]. Score: 0.043090719663538835.\n",
      "Experiment 30. Params: [-1.99942981  2.00962333]. Score: 0.011903759202936293.\n",
      "Experiment 31. Params: [-1.811084    2.03801964]. Score: 0.7579943871882109.\n",
      "Experiment 32. Params: [-1.94495639  2.00093585]. Score: 0.2180804974503916.\n",
      "Experiment 33. Params: [-1.99962944  2.00093585]. Score: 0.002417956921262565.\n",
      "Experiment 34. Params: [-1.99962944  2.00373015]. Score: 0.00521225885792953.\n",
      "Experiment 35. Params: [-1.94353177  2.05540779]. Score: 0.27809203905959334.\n",
      "Experiment 36. Params: [-1.96868386  2.02641319]. Score: 0.15069706281177941.\n",
      "Experiment 37. Params: [-1.99962944  2.01590373]. Score: 0.017385842982537536.\n",
      "Experiment 38. Params: [-1.99962944  2.00633266]. Score: 0.007814763984528739.\n",
      "Experiment 39. Params: [-1.97193973  2.00093585]. Score: 0.11238953967454846.\n",
      "Experiment 40. Params: [-1.98206688  2.01116367]. Score: 0.0825745385711536.\n",
      "Experiment 41. Params: [-1.99962944  2.01279588]. Score: 0.014277984607386163.\n",
      "Experiment 42. Params: [-1.99962944  2.00783551]. Score: 0.009317623161387623.\n",
      "Experiment 43. Params: [-1.95232548  2.02200432]. Score: 0.2104295517077195.\n",
      "Experiment 44. Params: [-1.98682411  2.00093585]. Score: 0.05346581810111317.\n",
      "Experiment 45. Params: [-1.99666299  2.00093585]. Score: 0.01427274196817363.\n",
      "Experiment 46. Params: [-1.99650856  2.00336225]. Score: 0.017315800234777967.\n",
      "Experiment 47. Params: [-1.98910485  2.02157219]. Score: 0.06503407064102307.\n",
      "Experiment 48. Params: [-1.99088495  2.01116473]. Score: 0.04754184495320901.\n",
      "Experiment 49. Params: [-1.99654105  2.00559746]. Score: 0.01942127346395539.\n",
      "Experiment 50. Params: [-0.17583333  2.45833333]. Score: 4.427415972222223.\n",
      "Experiment 51. Params: [-0.83916667  2.64166667]. Score: 3.937465972222222.\n",
      "Experiment 52. Params: [-0.5075      2.09166667]. Score: 3.834110416666667.\n",
      "Experiment 53. Params: [-1.83416667  2.825     ]. Score: 1.4608326388888888.\n",
      "Experiment 54. Params: [-1.5025      3.00833333]. Score: 2.7508270833333333.\n",
      "Experiment 55. Params: [-1.17083333  2.275     ]. Score: 2.9041493055555554.\n",
      "Experiment 56. Params: [-1.85134752  2.05476149]. Score: 0.6272738653838297.\n",
      "Experiment 57. Params: [-1.9226505   2.44133746]. Score: 0.7447525063970506.\n",
      "Experiment 58. Params: [-1.55190279  2.28350682]. Score: 1.875104554224563.\n",
      "Experiment 59. Params: [-1.99450685  2.1999849 ]. Score: 0.22192734214369692.\n",
      "Experiment 60. Params: [-1.99450685  2.00989279]. Score: 0.03183522592660282.\n",
      "Experiment 61. Params: [-1.36628902  2.00989279]. Score: 2.1431471026871955.\n",
      "Experiment 62. Params: [-1.62024516  2.00989279]. Score: 1.3846984014392967.\n",
      "Experiment 63. Params: [-1.99450685  2.09420842]. Score: 0.11615085938983594.\n",
      "Experiment 64. Params: [-1.99450685  2.00336484]. Score: 0.025307281930762482.\n",
      "Experiment 65. Params: [-1.03376897  2.00336484]. Score: 2.934686561337725.\n",
      "Experiment 66. Params: [-1.69759254  2.62563537]. Score: 1.7438149277571138.\n",
      "Experiment 67. Params: [-1.99450685  2.31598345]. Score: 0.3379258868794466.\n",
      "Experiment 68. Params: [-1.99450685  2.0061644 ]. Score: 0.028106840514747322.\n",
      "Experiment 69. Params: [-1.4519695   2.58769049]. Score: 2.4794750649959147.\n",
      "Experiment 70. Params: [-1.78136598  2.28267311]. Score: 1.109408360677075.\n",
      "Experiment 71. Params: [-1.99450685  2.05054901]. Score: 0.07249144749104586.\n",
      "Experiment 72. Params: [-1.99450685  2.00091477]. Score: 0.022857206137274044.\n",
      "Experiment 73. Params: [-1.70508703  2.1516796 ]. Score: 1.2443578320457074.\n",
      "Experiment 74. Params: [-1.83487323  2.17139898]. Score: 0.8046392072550006.\n",
      "Experiment 75. Params: [-1.99450685  2.14237327]. Score: 0.16431570694429043.\n",
      "Experiment 76. Params: [-1.99857835  2.00091477]. Score: 0.006599338826046797.\n",
      "Experiment 77. Params: [-1.74142047  2.00091477]. Score: 0.9683695054690995.\n",
      "Experiment 78. Params: [-1.9130797   2.00091477]. Score: 0.34104082509600175.\n",
      "Experiment 79. Params: [-1.99857835  2.03078319]. Score: 0.03646776344388858.\n",
      "Experiment 80. Params: [-1.99857835  2.00451587]. Score: 0.010200440084711104.\n",
      "Experiment 81. Params: [-1.50300802  2.00091477]. Score: 1.741881645230674.\n",
      "Experiment 82. Params: [-1.92818617  2.07147075]. Score: 0.3535688339097507.\n",
      "Experiment 83. Params: [-1.99857835  2.01855324]. Score: 0.024237807511986897.\n",
      "Experiment 84. Params: [-1.99695     2.00091477]. Score: 0.013105470370625927.\n",
      "Experiment 85. Params: [-1.80936724  2.00091477]. Score: 0.7271049579312971.\n",
      "Experiment 86. Params: [-1.95403161  2.00091477]. Score: 0.18267525139560226.\n",
      "Experiment 87. Params: [-1.99913802  2.00905379]. Score: 0.012500973524322667.\n",
      "Experiment 88. Params: [-1.99857835  2.00675624]. Score: 0.012440809928258911.\n",
      "Experiment 89. Params: [-1.95518656  2.03936885]. Score: 0.21661437131710937.\n",
      "Experiment 90. Params: [-1.97395331  2.01690917]. Score: 0.12041750470910317.\n",
      "Experiment 91. Params: [-1.99857835  2.01239335]. Score: 0.01807791730877062.\n",
      "Experiment 92. Params: [-1.99857835  2.00294609]. Score: 0.008630661879201362.\n",
      "Experiment 93. Params: [-1.97776572  2.04187969]. Score: 0.13032242779921566.\n",
      "Experiment 94. Params: [-1.9780104   2.00091477]. Score: 0.08838962611499879.\n",
      "Experiment 95. Params: [-1.99857835  2.0244349 ]. Score: 0.030119471604308945.\n",
      "Experiment 96. Params: [-1.99857835  2.01049217]. Score: 0.016176742067551153.\n",
      "Experiment 97. Params: [-1.9848086   2.00826539]. Score: 0.06880019347006305.\n",
      "Experiment 98. Params: [-1.98534153  2.02586626]. Score: 0.08428526072736764.\n",
      "Experiment 99. Params: [-1.99657781  2.00621968]. Score: 0.0198967375397463.\n",
      "Experiment 100. Params: [-1.99661897  2.00360423]. Score: 0.017116937834905865.\n",
      "Best value found: 0.002417956921262565\n",
      "Best solution found: [-1.99963  2.00094]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Test()\n",
    "maxeval = 100\n",
    "nsamples = 1\n",
    "nthreads = 1\n",
    "\n",
    "# Create a strategy and a controller\n",
    "controller = ThreadController()\n",
    "controller.strategy = SyncStrategyNoConstraints(worker_id=0, \n",
    "                                                data=data,\n",
    "                                                maxeval=maxeval, \n",
    "                                                nsamples=nsamples,\n",
    "                                                exp_design=LatinHypercube(dim=data.dim, npts=2*(data.dim+1)),\n",
    "                                                response_surface=RBFInterpolant(kernel=CubicKernel, maxp=maxeval),\n",
    "                                                sampling_method=CandidateDYCORS(data=data, numcand=100*data.dim))\n",
    "\n",
    "# Launch the threads and give them access to the objective function\n",
    "for _ in range(nthreads):\n",
    "    worker = BasicWorkerThread(controller, data.objfunction)\n",
    "    controller.launch_worker(worker)\n",
    "\n",
    "# Run the optimization strategy\n",
    "result = controller.run()\n",
    "\n",
    "print('Best value found: {0}'.format(result.value))\n",
    "print('Best solution found: {0}\\n'.format(\n",
    "    np.array_str(result.params[0], max_line_width=np.inf,\n",
    "                 precision=5, suppress_small=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prototype code for tuning a One Shot CNN on Fashion MNIST\n",
    "\n",
    "To use this approach for tuning Deep Learning models, we need classes that train and evaluate different architectures of Neural Networks. This prototype shows how such a class could look. It trains a simple CNN on five randomly chosen images of five randomly chosen classes and evaluates the found model on the test images of those same five classes. This process is repeated a number of times for every combination of parameter values to achieve a form of cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneShotPrototype():\n",
    "    \n",
    "    def __init__(self, dim=5):\n",
    "        \n",
    "        self.hyperparams = ['num_conv_layers', 'num_dense_layers', 'neurons_conv', 'neurons_dense', 'dropout_rate']\n",
    "        self.hyper_map = {self.hyperparams[i]:i for i in range(len(self.hyperparams))}\n",
    "        \n",
    "        # this may need a more intuitive structure\n",
    "        self.xlow = np.array([1, 1, 8, 8, 0.0])\n",
    "        self.xup = np.array([4, 4, 64, 128, 0.75])\n",
    "        self.continuous = np.arange(4,dim)\n",
    "        self.integer = np.arange(0,4)\n",
    "        self.dim = dim\n",
    "        \n",
    "        # fixed parameters\n",
    "        self.batchsize = 128\n",
    "        self.epochs = 200\n",
    "        self.nfolds = 5 # for cross validation\n",
    "        \n",
    "        # data\n",
    "        self.x_train, self.y_train = load_mnist(\"./Data/\", kind='train')\n",
    "        self.x_test, self.y_test = load_mnist(\"./Data/\", kind='test')\n",
    "        self.num_classes = self.y_test.shape[1]\n",
    "        \n",
    "        # logging results\n",
    "        #self.param_log = np.empty(shape=(,dim))\n",
    "        #self.scores_log = np.empty(shape=(,self.nfolds))\n",
    "        \n",
    "        # counter\n",
    "        self.exp_number = 0\n",
    "\n",
    "\n",
    "    def objfunction(self, params):\n",
    "        \"\"\" The overall objective function to provide to pySOT's black box optimization. \"\"\"\n",
    "        \n",
    "        self.exp_number += 1\n",
    "        print(\"--------------\\nExperiment {}.\\n--------------\".format(self.exp_number))\n",
    "        \n",
    "        def define_model(params):\n",
    "            \"\"\" Creates the Keras model based on given parameters. \"\"\"\n",
    "\n",
    "            model = Sequential()\n",
    "            \n",
    "            # add first convolutional layer and specify input shape\n",
    "            model.add(Conv2D(int(params[self.hyper_map['neurons_conv']]), \n",
    "                             kernel_size=(3,3), activation='relu', \n",
    "                             input_shape=(28,28,1), data_format=\"channels_last\"))\n",
    "            \n",
    "            # possibly add more\n",
    "            if int(params[self.hyper_map['num_conv_layers']]) > 1:\n",
    "                for l in range(1,int(params[0])):\n",
    "                    model.add(Conv2D(int(params[self.hyper_map['neurons_conv']]), (3, 3), activation='relu'))\n",
    "            \n",
    "            # max pool > dropout > flatten\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(params[self.hyper_map['dropout_rate']]))\n",
    "            model.add(Flatten())\n",
    "            \n",
    "            # add dense layers before the classification layer\n",
    "            for l in range(int(params[self.hyper_map['num_dense_layers']])):\n",
    "                model.add(Dense(int(params[self.hyper_map['neurons_dense']]), activation='relu'))\n",
    "            \n",
    "            # classification layer\n",
    "            model.add(Dense(self.num_classes, activation='softmax'))\n",
    "            \n",
    "            # compile and return\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                          optimizer='rmsprop',\n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            # create data generator, later including augmentations\n",
    "            datagen = ImageDataGenerator()\n",
    "\n",
    "            return model, datagen\n",
    "            \n",
    "\n",
    "        def cross_validate(x, y, xtest, ytest, params, n):\n",
    "            \"\"\" Cross validate with random sampling. \"\"\"\n",
    "            \n",
    "            print(\"Cross validating..\")\n",
    "            scores = []\n",
    "            for i in range(n):\n",
    "                x_target_labeled, y_target, x_test, y_test, _, _, _ = \\\n",
    "                    split_and_select_random_data(x, y, xtest, ytest,\n",
    "                                                 num_target_classes=5, num_examples_per_class=1)\n",
    "                model, datagen = define_model(params)\n",
    "                print(\"fit {}:\".format(i+1))\n",
    "                # fits the model on batches with real-time data augmentation:\n",
    "                model.fit_generator(datagen.flow(x_target_labeled, y_target, batch_size=x_target_labeled.shape[0]), \n",
    "                                    steps_per_epoch=1, epochs=self.epochs, verbose=0)\n",
    "\n",
    "                loss, accuracy = model.evaluate(x_test, y_test, verbose=0, batch_size=y.shape[0])\n",
    "                print(\"test accuracy: {}%.\".format(round(accuracy*100, 2)))\n",
    "                scores.append(accuracy)\n",
    "            \n",
    "            return scores\n",
    "        \n",
    "        \n",
    "        print(\"params: {}.\".format(params))\n",
    "        scores = cross_validate(self.x_train, self.y_train, self.x_test, self.y_test, params, self.nfolds)\n",
    "        print(\"Scores: {}.\\nMean: {}%. Standard deviation: {}%\".format(scores, round(np.mean(scores)*100, 2), round(np.std(scores)*100, 2)))\n",
    "        \n",
    "        # to minimize, return this to pySOT\n",
    "        return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the prototype class\n",
    "Test the prototype class for a single set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-58f4e5f527b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneShotPrototype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-1ba79ac97bfe>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dim)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./Data/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./Data/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "test = OneShotPrototype()\n",
    "score = test.objfunction([2, 2, 16, 16, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try tuning the model with HORD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = OneShotPrototype()\n",
    "maxeval = 15\n",
    "nsamples = 1 # one experiment at the time\n",
    "\n",
    "# create the controller\n",
    "controller = SerialController(data.objfunction)\n",
    "# experiment design\n",
    "exp_des = LatinHypercube(dim=data.dim, npts=2*data.dim+1)\n",
    "# Use a cubic RBF interpolant with a linear tail\n",
    "surrogate = RBFInterpolant(kernel=CubicKernel, tail=LinearTail, maxp=maxeval)\n",
    "# Use DYCORS with 100d candidate points\n",
    "adapt_samp = CandidateDYCORS(data=data, numcand=100*data.dim)\n",
    "\n",
    "strategy = SyncStrategyNoConstraints(worker_id=0, data=data, maxeval=maxeval, nsamples=1,\n",
    "                                     exp_design=exp_des, response_surface=surrogate,\n",
    "                                     sampling_method=adapt_samp)\n",
    "controller.strategy = strategy\n",
    "\n",
    "# Run the optimization strategy\n",
    "start_time = datetime.now()\n",
    "result = controller.run()\n",
    "\n",
    "print('Best value found: {0}'.format(result.value))\n",
    "print('Best solution found: {0}\\n'.format(\n",
    "    np.array_str(result.params[0], max_line_width=np.inf,\n",
    "                 precision=5, suppress_small=True)))\n",
    "\n",
    "millis = int(round(time.time() * 1000))\n",
    "print('Started: '+str(start_time)+'. Ended: ' + str(datetime.now()) + ' (' + str(millis) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that using the multi-thread controller does not work for this class, because this conflicts with Keras, which already uses all available cores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
